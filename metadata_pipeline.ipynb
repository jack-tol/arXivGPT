{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from sickle import Sickle\n",
    "from requests.exceptions import HTTPError, RequestException\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Configure logging\n",
    "def setup_logging():\n",
    "    logging.basicConfig(filename='arxiv_download.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def download_metadata(from_date, until_date):\n",
    "    connection = Sickle('http://export.arxiv.org/oai2')\n",
    "    logging.info('Getting papers...')\n",
    "    params = {'metadataPrefix': 'arXiv', 'from': from_date, 'until': until_date, 'ignore_deleted': True}\n",
    "    data = connection.ListRecords(**params)\n",
    "    logging.info('Papers retrieved.')\n",
    "\n",
    "    iters = 0\n",
    "    errors = 0\n",
    "\n",
    "    with open('arXiv_metadata_raw.xml', 'a+', encoding=\"utf-8\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                record = next(data).raw\n",
    "                f.write(record)\n",
    "                f.write('\\n')\n",
    "                errors = 0\n",
    "                iters += 1\n",
    "                if iters % 1000 == 0:\n",
    "                    logging.info(f'{iters} Processing Attempts Made Successfully.')\n",
    "\n",
    "            except HTTPError as e:\n",
    "                handle_http_error(e)\n",
    "\n",
    "            except RequestException as e:\n",
    "                logging.error(f'RequestException: {e}')\n",
    "                raise\n",
    "\n",
    "            except StopIteration:\n",
    "                logging.info(f'Metadata For The Specified Period, {from_date} - {until_date} Downloaded.')\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                errors += 1\n",
    "                logging.error(f'Unexpected error: {e}')\n",
    "                if errors > 5:\n",
    "                    logging.critical('Too many consecutive errors, stopping the harvester.')\n",
    "                    raise\n",
    "\n",
    "def handle_http_error(e):\n",
    "    if e.response.status_code == 503:\n",
    "        retry_after = e.response.headers.get('Retry-After', 30)\n",
    "        logging.warning(f\"HTTPError 503: Server busy. Retrying after {retry_after} seconds.\")\n",
    "        time.sleep(int(retry_after))\n",
    "    else:\n",
    "        logging.error(f'HTTPError: Status code {e.response.status_code}')\n",
    "        raise e\n",
    "\n",
    "def parse_xml_to_df(xml_file):\n",
    "    with open(xml_file, 'r', encoding='utf-8') as file:\n",
    "        xml_content = file.read()\n",
    "    \n",
    "    if not xml_content.strip().startswith('<root>'):\n",
    "        xml_content = f\"<root>{xml_content}</root>\"\n",
    "\n",
    "    root = ET.ElementTree(ET.fromstring(xml_content)).getroot()\n",
    "    records = []\n",
    "    ns = {\n",
    "        'oai': 'http://www.openarchives.org/OAI/2.0/',\n",
    "        'arxiv': 'http://arxiv.org/OAI/arXiv/'\n",
    "    }\n",
    "    \n",
    "    for record in root.findall('oai:record', ns):\n",
    "        data = {}\n",
    "        header = record.find('oai:header', ns)\n",
    "        data['identifier'] = header.find('oai:identifier', ns).text\n",
    "        data['datestamp'] = header.find('oai:datestamp', ns).text\n",
    "        data['setSpec'] = [elem.text for elem in header.findall('oai:setSpec', ns)]\n",
    "        \n",
    "        metadata = record.find('oai:metadata/arxiv:arXiv', ns)\n",
    "        data['id'] = metadata.find('arxiv:id', ns).text\n",
    "        data['created'] = metadata.find('arxiv:created', ns).text\n",
    "        data['updated'] = metadata.find('arxiv:updated', ns).text if metadata.find('arxiv:updated', ns) is not None else None\n",
    "        data['authors'] = [\n",
    "            (author.find('arxiv:keyname', ns).text if author.find('arxiv:keyname', ns) is not None else None,\n",
    "             author.find('arxiv:forenames', ns).text if author.find('arxiv:forenames', ns) is not None else None)\n",
    "            for author in metadata.findall('arxiv:authors/arxiv:author', ns)\n",
    "        ]\n",
    "        data['title'] = metadata.find('arxiv:title', ns).text\n",
    "        data['categories'] = metadata.find('arxiv:categories', ns).text\n",
    "        data['comments'] = metadata.find('arxiv:comments', ns).text if metadata.find('arxiv:comments', ns) is not None else None\n",
    "        data['report_no'] = metadata.find('arxiv:report-no', ns).text if metadata.find('arxiv:report-no', ns) is not None else None\n",
    "        data['journal_ref'] = metadata.find('arxiv:journal-ref', ns).text if metadata.find('arxiv:journal-ref', ns) is not None else None\n",
    "        data['doi'] = metadata.find('arxiv:doi', ns).text if metadata.find('arxiv:doi', ns) is not None else None\n",
    "        data['license'] = metadata.find('arxiv:license', ns).text if metadata.find('arxiv:license', ns) is not None else None\n",
    "        data['abstract'] = metadata.find('arxiv:abstract', ns).text.strip() if metadata.find('arxiv:abstract', ns) is not None else None\n",
    "        \n",
    "        records.append(data)\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    # drops all columns which are not relevant for this application\n",
    "    df = df[['datestamp', 'id', 'created', 'authors', 'title']].copy()\n",
    "\n",
    "    # renames fields to more accurately align with what they represent \n",
    "    df.rename(columns={\n",
    "        'datestamp': 'last_edited',\n",
    "        'id': 'document_id',\n",
    "        'created': 'date_created'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # converts the 'title' and 'authors' fields to the string datatype\n",
    "    df.loc[:, 'title'] = df['title'].astype(str)\n",
    "    df.loc[:, 'authors'] = df['authors'].astype(str)\n",
    "\n",
    "    # replace any double spaces with single spaces in the 'title' and 'authors' fields\n",
    "    df.loc[:, 'title'] = df['title'].str.replace('  ', ' ', regex=True)\n",
    "    df.loc[:, 'authors'] = df['authors'].str.replace('  ', ' ', regex=True)\n",
    "\n",
    "    # checks to make sure that each record within the 'document_id' field starts with an integer\n",
    "    df = df[df['document_id'].str.match('^\\d')]\n",
    "\n",
    "    # removes any line breaks from the 'title' field\n",
    "    df.loc[:, 'title'] = df['title'].str.replace('\\n', '', regex=True)\n",
    "    \n",
    "    # removes any square brackets, parentheses, single quotes, and double quotes from the 'authors' field\n",
    "    df.loc[:, 'authors'] = df['authors'].str.replace('[\\[\\]\\'\"()]', '', regex=True)\n",
    "\n",
    "    # defines a flip_names function which changes the 'authors' format from 'Lastname, Firstname' to 'Firstname, Lastname' and limits the amount of authors to 10\n",
    "    def flip_names(authors):\n",
    "        author_list = authors.split(', ')\n",
    "        flipped_authors = []\n",
    "        for i in range(0, len(author_list), 2):\n",
    "            if i + 1 < len(author_list):\n",
    "                flipped_authors.append(f\"{author_list[i + 1]} {author_list[i]}\")\n",
    "        return ', '.join(flipped_authors[:10])\n",
    "\n",
    "    # applies the flip_names function to each record in the 'authors' field\n",
    "    df.loc[:, 'authors'] = df['authors'].apply(flip_names)\n",
    "    \n",
    "    # converts the 'last_edited' and 'date_created' fields to the datatime datatype\n",
    "    df.loc[:, 'last_edited'] = pd.to_datetime(df['last_edited'])\n",
    "    df.loc[:, 'date_created'] = pd.to_datetime(df['date_created'])\n",
    "    \n",
    "    # filter for rows where 'last_edited' is the same as 'date_created' + 1 day\n",
    "    df = df[df['last_edited'] == df['date_created'] + pd.Timedelta(days=1)]\n",
    "    \n",
    "    # concatenate 'title' and 'authors' into a new field 'title_by_authors'\n",
    "    df.loc[:, 'title_by_authors'] = df['title'] + ' by ' + df['authors']\n",
    "    \n",
    "    # drops the original 'title', 'authors', 'date_created', and 'last_edited' fields\n",
    "    df.drop(['title', 'authors', 'date_created', 'last_edited'], axis=1, inplace=True)\n",
    "    \n",
    "    # exports the processed dataframe to a csv file\n",
    "    df.to_csv('metadata_processed.csv', index=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def upload_to_pinecone(df, vector_store):\n",
    "    texts = df['title_by_authors'].tolist()\n",
    "    metadatas = df[['document_id']].to_dict(orient='records')\n",
    "    vector_store.add_texts(texts=texts, metadatas=metadatas)\n",
    "\n",
    "def main():\n",
    "    setup_logging()\n",
    "    from_date = '2024-03-05'\n",
    "    until_date = '2024-03-06'\n",
    "    download_metadata(from_date, until_date)\n",
    "    xml_file = 'arXiv_metadata_raw.xml'\n",
    "    df = parse_xml_to_df(xml_file)\n",
    "    os.remove(xml_file)\n",
    "    df = preprocess_dataframe(df)\n",
    "    if not df.empty:\n",
    "        PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "        embeddings_model = HuggingFaceEmbeddings()\n",
    "        index_name = \"test-index\"\n",
    "        vector_store = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embeddings_model)\n",
    "        upload_to_pinecone(df, vector_store)\n",
    "    else:\n",
    "        logging.error(\"DataFrame is empty. Skipping upload.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
